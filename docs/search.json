{
  "articles": [
    {
      "path": "about.html",
      "title": "About Our Group",
      "description": "Who we are and what we are intreseted in?",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-05-09T15:46:10+02:00"
    },
    {
      "path": "Blog.html",
      "title": "Blog Posts on Contreversial Topics",
      "description": "😱\n",
      "author": [
        {
          "name": "Chris Donkin vs. Nathan Evans",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nKlaus Fiedler, editor in chief of the journal Perspectives on Psychological Science: An incompetent editor or a shameless racist?\n\n\n\n",
      "last_modified": "2023-05-09T15:46:11+02:00"
    },
    {
      "path": "index.html",
      "title": "Chair of Computational Modeling in Psychology",
      "author": [],
      "contents": "\n\n          \n          \n          Computational Modeling in Psychology: LMU\n          \n          \n          Home\n          About\n          Blog\n          \n          \n          Teaching\n           \n          ▾\n          \n          \n          Basics in statistics - Linear Models\n          Basics in statistics - Distributions\n          Basics in statistics - Central Limit Theorem\n          Basics in statistics - P-Hacking\n          \n          \n          ☰\n          \n          \n      \n        \n          Chair of Computational Modeling in Psychology\n          \n            \n                \n                  \n                     LMU Page\n                  \n                \n              \n                            \n                \n                  \n                     Github\n                  \n                \n              \n                            \n                \n                  \n                     Twitter\n                  \n                \n              \n                          \n        \n\n        \n          \n      \n      \n        \n          \n          News\n          \n          \n          New Publications\n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Chair of Computational Modeling in Psychology\n            \n            \n              \n                \n                                    \n                    \n                       LMU Page\n                    \n                  \n                                    \n                    \n                       Github\n                    \n                  \n                                    \n                    \n                       Twitter\n                    \n                  \n                                  \n              \n            \n            \n              \n              News\n              \n              \n              New Publications\n              \n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-05-10T13:07:54+02:00"
    },
    {
      "path": "p_hacking_sim.html",
      "title": "P-Hacking",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIn statistical analysis, it’s important to have a solid understanding of the underlying data generating process before drawing any meaningful conclusions.\nTo illustrate this concept, the following code chunk simulates five datasets (Daten1-Daten5) with 10 (n = 10) random samples.\nThe code generates the datasets using the rnorm() function, and ensures that the data is normally distributed with a mean of 0 (which is the default value if not otherwise specified) and standard deviation of 1 (which is the default value if not otherwise specified).\nBy simulating datasets in this way, we can get a better understanding of what we would expect to see if we were to conduct statistical tests on random samples.\nAs the data is generated RANDOMALY from the same distribution (i.e., with the same mean and \\(\\sigma\\)), we shouldn’t expect to find any actual differences when conducting statistical tests.\nThis also means that we should operate under the assumption that the \\(H_{0}\\) is true.\nRight?\nIf we do…. we should conclude that we are dealing with a false positive (Type I Error).\nWhats the probability of obsering Type I error?\nLet’s take a closer look at the code and the plots.\n\n\n# UB 3: Aufgabe 1\n\n# Stichprobengröße pro Experiment und pro Gruppe\nn <- 10\n# Der Seed stellt sicher, dass Sie die gleichen simulierten Daten erhalten \n# wie in der Musterlösung\nset.seed(21)\n\n# Simuliere Datensätze der 5 Experimente\n# x1: Konzentrationsleistung der Gruppe mit Präsentation emotional belastender Stimuli\n# x2: Konzentrationsleistung der Kontrollgruppe\nDaten1 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\nDaten2 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\nDaten3 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\nDaten4 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\nDaten5 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\n\n\nLets look at some statistics.\nWe start with comoputeing the Cohen’s \\(d\\) for the comparison between the first data set and the 4 others.\n\n\n# Berechnung von Cohens d mit dem effsize Paket\nlibrary(effsize)\nd1 <- cohen.d(Daten1$x1, Daten1$x2, na.rm = TRUE)$estimate\nd2 <- cohen.d(Daten2$x1, Daten2$x2, na.rm = TRUE)$estimate\nd3 <- cohen.d(Daten3$x1, Daten3$x2, na.rm = TRUE)$estimate\nd4 <- cohen.d(Daten4$x1, Daten4$x2, na.rm = TRUE)$estimate\nd5 <- cohen.d(Daten5$x1, Daten5$x2, na.rm = TRUE)$estimate\nds <- c(d1, d2, d3, d4, d5)\n\n\nWe proceed by computing the weights for each of the effect sizes obtained above.\nThe formula is \\(w_{i} = \\frac{1}{\\frac{2*n}{n^2} + \\frac{d^2}{4*n}}\\)\n\n\n# Berechnung der Gewichte\nw1 <- 1 / (((2*n)/(n^2)) + ((d1^2) / (4*n)))\nw2 <- 1 / (((2*n)/(n^2)) + ((d2^2) / (4*n)))\nw3 <- 1 / (((2*n)/(n^2)) + ((d3^2) / (4*n)))\nw4 <- 1 / (((2*n)/(n^2)) + ((d4^2) / (4*n)))\nw5 <- 1 / (((2*n)/(n^2)) + ((d5^2) / (4*n)))\nws <- c(w1, w2, w3, w4, w5)\n\n\nAnd the CI for the sum of weighted effect sizes\n\n[1] -0.6135862\n[1] 0.1785985\n\nAnd finally, lets look if we encounter a somewhat surprising Type I error in t-tests\n\n\n    Two Sample t-test\n\ndata:  Daten1$x1 and Daten1$x2\nt = 0.23431, df = 18, p-value = 0.8174\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.9770869  1.2223869\nsample estimates:\n mean of x  mean of y \n0.19766656 0.07501655 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -2.0513, df = 18, p-value = 0.05508\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.77519685  0.02122661\nsample estimates:\n mean of x  mean of y \n-0.3573462  0.5196389 \n\n    Two Sample t-test\n\ndata:  Daten3$x1 and Daten3$x2\nt = -0.33343, df = 18, p-value = 0.7427\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.0944010  0.7946067\nsample estimates:\nmean of x mean of y \n0.1873067 0.3372038 \n\n    Two Sample t-test\n\ndata:  Daten4$x1 and Daten4$x2\nt = -0.060552, df = 18, p-value = 0.9524\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.130185  1.066863\nsample estimates:\n  mean of x   mean of y \n-0.12762999 -0.09596875 \n\n    Two Sample t-test\n\ndata:  Daten5$x1 and Daten5$x2\nt = -0.36782, df = 18, p-value = 0.7173\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.0007232  0.7025292\nsample estimates:\n  mean of x   mean of y \n-0.08006336  0.06903360 \n\nCohen's d\n\nd estimate: -0.9173571 (large)\n95 percent confidence interval:\n      lower       upper \n-1.90510021  0.07038597 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -1.2058, df = 20, p-value = 0.242\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.6471444  0.4404262\nsample estimates:\n mean of x  mean of y \n-0.3985774  0.2047817 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -1.1409, df = 22, p-value = 0.2662\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.4795881  0.4294105\nsample estimates:\n mean of x  mean of y \n-0.3669289  0.1581599 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -1.2226, df = 24, p-value = 0.2333\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.3977146  0.3577739\nsample estimates:\n mean of x  mean of y \n-0.3359674  0.1840029 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -1.5085, df = 26, p-value = 0.1435\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.4210316  0.2180937\nsample estimates:\n mean of x  mean of y \n-0.3590989  0.2423700 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -1.4042, df = 28, p-value = 0.1713\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.3284528  0.2478866\nsample estimates:\n mean of x  mean of y \n-0.2636867  0.2765964 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -1.7746, df = 30, p-value = 0.08612\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.41184668  0.09902395\nsample estimates:\n mean of x  mean of y \n-0.3381728  0.3182385 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -1.9774, df = 32, p-value = 0.05666\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.44728953  0.02145603\nsample estimates:\n mean of x  mean of y \n-0.4282965  0.2846203 \n\n\n\n# FEHLER: RESEARCHER DEGREES OF FREEDOM!!!\n# Boxplot aus Experiment 2 zeigt einen \"Ausreißer\" in Gruppe 2\nboxplot(Daten2$x1, Daten2$x2)\n\n\n# Ersetze Ausreißer durch NA\nDaten2[11, \"x2\"] <- NA\n# Berechne erneut den p-Wert\nt.test(Daten2$x1, Daten2$x2, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -2.9672, df = 31, p-value = 0.005746\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.5434009 -0.2859839\nsample estimates:\n mean of x  mean of y \n-0.4282965  0.4863959 \n\n# Berechne d und w für Experiment 2 neu\nd2 <- cohen.d(Daten2$x1, Daten2$x2, na.rm = TRUE)$estimate\nw2 <- 1 / (((16+17)/(16*17)) + ((d2^2) / (2*(16+17))))\n\n# FEHLER: PUBLICATION BIAS!!!\n# Berechne metaanalytischen Hypothesentest mit ungerichteter H1\n# nur für Experimente 2, 3, 4, 5\nds <- c(d2, d3, d4, d5)\nws <- c(w2, w3, w4, w5)\nd <- sum(ds * ws) / sum(ws)\nt_statistik <- d * sqrt(sum(ws))\n2 * pnorm(t_statistik)\n\n[1] 0.050712\n\n# FEHLER: HARKING!!!\n# Berechne Hypothesentest mit linksgerichteter H1\npnorm(t_statistik)\n\n[1] 0.025356\n\nlibrary(pwr)\n# FEHLER: POST HOC POWER!!!\n# Berechne Stichprobenumfang mit dem Schätzwert d2 als wahres delta\npwr.t.test(power = 0.8, sig.level = 0.05, d = -1.034, alternative = \"less\")\n\n\n     Two-sample t test power calculation \n\n              n = 12.30187\n              d = -1.034\n      sig.level = 0.05\n          power = 0.8\n    alternative = less\n\nNOTE: n is number in *each* group\n\n# Konfidenzintervall für delta basierend auf Experiment 2 (nach p-Hacking)\ncohen.d(Daten2$x1, Daten2$x2, na.rm = TRUE)\n\n\nCohen's d\n\nd estimate: -1.033534 (large)\n95 percent confidence interval:\n     lower      upper \n-1.7898287 -0.2772395 \n\nA littile background story:\nOnce upon a time, there was a researcher named Bob who was tasked with analyzing data for his company. Bob had a lot of pressure on his shoulders, as his boss had promised to deliver some groundbreaking results by the end of the week. Bob knew he had to produce statistical results, but he was also aware that statistical analysis could be quite tricky.\nDetermined to get the job done, Bob started analyzing his data. After every single observation, Bob would check to see if the results were significant. If they weren’t, he would move on to the next observation.\nHowever, as Bob was also a lazy p-hacker, he wanted to know how much time he will have to invest in this whole mess. He therefore created a function that simulates this process and will determines after how many observations (approximately) a significant p-value will be found.\nBad, lazy Bob….😳\nNote: this simulation was created by the hard-working, never-p-hacking 😉 Elisabeth Kraus.\nHere is the code for the simulation\nHere is the simulation for p-hacking with \\(p \\le .05\\).\n\n\n\nHere is the simulation for p-hacking with \\(p \\le .005\\).\n\n\n\n\n\n\n",
      "last_modified": "2023-05-10T13:06:50+02:00"
    },
    {
      "path": "Stat_basics_central.html",
      "title": "Central Limit Theorm",
      "description": ".\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nCentral Limit Theorm\nThe central limit theorem is a fundamental concept in statistics that states that as the sample size increases, the distribution of the sample means will approach a normal distribution, regardless of the underlying distribution of the population.\nThis is true for any independent, identically distributed random variables with a finite mean and variance. The central limit theorem is essential because it allows us to make inferences about a population based on a sample.\nThe basic equation for the central limit theorem is:\n\\(z = (x̄ - μ) / (σ / √n)\\)\nwhere \\(z\\) is the standard normal random variable, \\(x̄\\) is the sample mean, μ is the population mean, σ is the population standard deviation, and n is the sample size.\nA simulation plot of the central limit theorem can demonstrate how the sample mean distribution approaches a normal distribution as the sample size increases. The plot can show different sample sizes on the x-axis and the sample mean distribution on the y-axis. The simulation can be run using a non-normal population distribution, such as the uniform or exponential distribution, and show that as the sample size increases, the distribution of the sample means becomes increasingly normal. The plot can also illustrate how the standard deviation of the sample means decreases as the sample size increases, indicating more precise estimates of the population mean. This visualization can help to reinforce the concept of the central limit theorem and its practical implications in statistical analysis.\n\n\nknitr::include_graphics(\"CentralTheorm_ND_sim.png\")\n\n\n\nWhy do we need central theorm?\nIntuitively, we can think of the distribution of sample means as a way of characterizing the variability that we would expect to see in the sample means if we were to take many different samples from the same population. If we were to take multiple samples of the same size from a population and calculate the sample mean for each sample, we would end up with a range of different values. The distribution of these sample means would give us a sense of how much variability we can expect in the sample mean, which in turn provides information about the uncertainty in our estimate of the population mean.\nThe distribution of sample means is important because it allows us to make statements about the population mean with a certain degree of confidence. By calculating the standard error of the mean and constructing confidence intervals around the sample mean, we can estimate the range of values that the population mean is likely to fall within with a certain level of confidence. This is a powerful tool in statistical inference and is used in a wide range of fields, from medicine to finance to social sciences.\n\n\n\n",
      "last_modified": "2023-05-09T15:46:14+02:00"
    },
    {
      "path": "Stat_basics_distr.html",
      "title": "Distributions",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nThe Normal Distribution\nThe normal distribution, also known as the Gaussian distribution, is a probability distribution that is widely used in statistics and science. It is characterized by a bell-shaped curve, which is symmetrical and has a single peak at the center.\nThe normal distribution is defined by two parameters:\nThe mean, which determines the center of the curve, and\nThe standard deviation, which determines the width of the curve.\nThe equation for the normal distribution is:\n\\(f(x) = (1/σ√(2π)) e^{(-(x-μ)²/(2σ²))}\\)\nwhere \\(f(x)\\) is the probability density function of the normal distribution, \\(x\\) is the random variable, μ is the mean, σ is the standard deviation, and e is the mathematical constant, known as Euler’s number.\nIntuitively, we can think of the normal distribution as a way of describing the variability of a set of data.\nIf we have a set of data that is normally distributed, we can use the mean and standard deviation to understand the characteristics of the data.\n-> The mean tells us the central value around which the data is centered, while the standard deviation tells us the spread or variability of the data around the mean.\nOne of the key features of the normal distribution is the 68-95-99.7 rule, also known as the empirical rule or the three-sigma rule. This rule states that approximately 68% of the data falls within one standard deviation of the mean, 95% of the data falls within two standard deviations of the mean, and 99.7% of the data falls within three standard deviations of the mean. This rule is important because it allows us to make inferences about the likelihood of observing certain values in a normally distributed dataset.\nThe normal distribution is widely used in statistics and science because it is a useful model for many natural phenomena. For example, many physical and biological measurements, such as height, weight, and IQ scores, are normally distributed in the population. The normal distribution also plays a key role in statistical hypothesis testing and estimation, as many statistical tests assume normality of the underlying data.\n\n\nlibrary(knitr)\nknitr::include_graphics(\"Emp_Rule_normalDist.png\")\n\n\n\nBivariate Normal Distribution\nBivariate normal distribution is a statistical model that describes the joint distribution of two random variables that are normally distributed. It is used to analyze the relationship between two continuous variables, such as height and weight or income and education level. The distribution is characterized by two parameters: the mean and the covariance.\nThe mean represents the center of the distribution, while the covariance measures the degree to which the two variables vary together.\nA 3D simulation image of a bivariate normal distribution (see below) shows two variables plotted on the x and y-axis, with the height of the surface representing the probability density of each combination of values.\nThe surface is shaped like an elliptical mound, with the peak at the mean of the distribution and the orientation of the ellipse determined by the covariance.\nThe shape of the ellipse reflects the degree to which the two variables are correlated, with a more circular shape indicating low correlation and a more elongated shape indicating high correlation.\nThe visualization was created as a tool to help statistics students to understand the relationship between two variables in a more intuitive way.\nI created this plot to illustrate the Bivariat normal distribution in Python (here is the link to the code\n\n\nknitr::include_graphics(\"BIVARNORMAL.png\")\n\n\n\n\n\n\n",
      "last_modified": "2023-05-10T09:40:47+02:00"
    },
    {
      "path": "Stat_basics_lm.html",
      "title": "Basic concepts in Statistics",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHello and welcome to my blog post about linear models in statistics!\nLinear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables. They are widely used in fields such as economics, engineering, and social sciences to make predictions and understand the impact of one variable on another. In this post, we’ll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.\nBefore we dive into the Shiny app, let’s first define what a linear model is. A linear model is a mathematical equation that represents a linear relationship between two or more variables. The simplest form of a linear model is a straight line equation of the form:\n\\(y_{i} = \\beta_{0} + \\beta_{1} \\times X_{1}\\)\nWhere Y is the dependent variable, X is the independent variable, a is the intercept, and b is the slope of the line. The slope represents the change in Y for every unit increase in X, while the intercept is the value of Y when X is zero.\nLinear models can be extended to include more than one independent variable, and the equation becomes:\n\\(y{i} = a + \\beta_{1} \\times X_{1} + \\beta_{2} \\times X_{2} + … + \\beta_{n} \\times X_{n}\\)\nWhere Y is the dependent variable, X1, X2, …, Xn are the independent variables, a is the intercept, and b1, b2, …, bn are the slopes of the respective variables.\nNow let’s move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:\nChoose the variables from the dropdown menu.\nObserve how the line changes to fit the data points better.\nObserve how the changes in:\nIntercept\nSlope(s)\np-values\nR-squared value as you adjust the model.\nThe app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.\nThe data Im using here is from a built-in dataset in R called “mtcar”.\n\n\nlibrary(knitr)\nkable(head(mtcars))\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\nAfter looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model – the line with the lowest RSS)\n\n\n\n\nsomehting\n\n\n\n\nIn conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don’t hesitate to leave any comments or questions below!\n\n\n\n",
      "last_modified": "2023-05-09T15:46:15+02:00"
    },
    {
      "path": "Teaching.html",
      "author": [],
      "contents": "\ntitle: “Untitled”\ndescription: |\nA new article created using the Distill format.\nauthor:\n- name: Nora Jones\nurl: https://example.com/norajones\naffiliation: Spacely Sprockets\naffiliation_url: https://example.com/spacelysprokets\ndate: “2023-05-09”\noutput:\noutput: distill::distill_article\n\n\n\n",
      "last_modified": "2023-05-09T15:46:16+02:00"
    }
  ],
  "collections": []
}
