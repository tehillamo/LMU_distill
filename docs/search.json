{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Our Team ",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-05-09T12:45:38+02:00"
    },
    {
      "path": "Blog.html",
      "title": "Blog Posts on Contreversial Topics",
      "description": "😱\n",
      "author": [
        {
          "name": "Chris Donkin vs. Nathan Evans",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nKlaus Fiedler, editor in chief of the journal Perspectives on Psychological Science: An incompetent editor or a shameless racist?\n\n\n\n",
      "last_modified": "2023-05-09T12:45:39+02:00"
    },
    {
      "path": "index.html",
      "title": "Chair of Computational Modeling in Psychology",
      "author": [],
      "contents": "\n\n          \n          \n          Computational Modeling in Psychology: LMU\n          \n          \n          Home\n          About\n          Blog\n          \n          \n          Teaching\n           \n          ▾\n          \n          \n          Basics in statistics - Linear Models\n          Basics in statistics - Distributions\n          \n          \n          ☰\n          \n          \n      \n        \n          Chair of Computational Modeling in Psychology\n          \n            \n                \n                  \n                     LMU Page\n                  \n                \n              \n                            \n                \n                  \n                     Github\n                  \n                \n              \n                            \n                \n                  \n                     Twitter\n                  \n                \n              \n                          \n        \n\n        \n          \n      \n      \n        \n          \n          News\n          \n          \n          New Publications\n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Chair of Computational Modeling in Psychology\n            \n            \n              \n                \n                                    \n                    \n                       LMU Page\n                    \n                  \n                                    \n                    \n                       Github\n                    \n                  \n                                    \n                    \n                       Twitter\n                    \n                  \n                                  \n              \n            \n            \n              \n              News\n              \n              \n              New Publications\n              \n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-05-09T15:01:00+02:00"
    },
    {
      "path": "Stat_basics_distr.html",
      "title": "Distributions",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nThe Normal Distribution\nThe normal distribution, also known as the Gaussian distribution, is a probability distribution that is widely used in statistics and science. It is characterized by a bell-shaped curve, which is symmetrical and has a single peak at the center.\nThe normal distribution is defined by two parameters:\nThe mean, which determines the center of the curve, and\nThe standard deviation, which determines the width of the curve.\nThe equation for the normal distribution is:\n\\(f(x) = (1/σ√(2π)) e^{(-(x-μ)²/(2σ²))}\\)\nwhere \\(f(x)\\) is the probability density function of the normal distribution, \\(x\\) is the random variable, μ is the mean, σ is the standard deviation, and e is the mathematical constant, known as Euler’s number.\nIntuitively, we can think of the normal distribution as a way of describing the variability of a set of data.\nIf we have a set of data that is normally distributed, we can use the mean and standard deviation to understand the characteristics of the data.\n-> The mean tells us the central value around which the data is centered, while the standard deviation tells us the spread or variability of the data around the mean.\nOne of the key features of the normal distribution is the 68-95-99.7 rule, also known as the empirical rule or the three-sigma rule. This rule states that approximately 68% of the data falls within one standard deviation of the mean, 95% of the data falls within two standard deviations of the mean, and 99.7% of the data falls within three standard deviations of the mean. This rule is important because it allows us to make inferences about the likelihood of observing certain values in a normally distributed dataset.\nThe normal distribution is widely used in statistics and science because it is a useful model for many natural phenomena. For example, many physical and biological measurements, such as height, weight, and IQ scores, are normally distributed in the population. The normal distribution also plays a key role in statistical hypothesis testing and estimation, as many statistical tests assume normality of the underlying data.\n\n\nlibrary(knitr)\nknitr::include_graphics(\"Emp_Rule_normalDist.png\")\n\n\n\nBivariate Normal Distribution\nBivariate normal distribution is a statistical model that describes the joint distribution of two random variables that are normally distributed. It is used to analyze the relationship between two continuous variables, such as height and weight or income and education level. The distribution is characterized by two parameters: the mean and the covariance.\nThe mean represents the center of the distribution, while the covariance measures the degree to which the two variables vary together.\nA 3D simulation image of a bivariate normal distribution (see below) shows two variables plotted on the x and y-axis, with the height of the surface representing the probability density of each combination of values.\nThe surface is shaped like an elliptical mound, with the peak at the mean of the distribution and the orientation of the ellipse determined by the covariance.\nThe shape of the ellipse reflects the degree to which the two variables are correlated, with a more circular shape indicating low correlation and a more elongated shape indicating high correlation.\nThe visualization was created as a tool to help statistics students to understand the relationship between two variables in a more intuitive way.\nI created this plot to illustrate the Bivariat normal distribution in Python (here is the link to the code\n\n\nknitr::include_graphics(\"BIVARNORMAL.png\")\n\n\n\nCentral Limit Theorm\nThe central limit theorem is a fundamental concept in statistics that states that as the sample size increases, the distribution of the sample means will approach a normal distribution, regardless of the underlying distribution of the population.\nThis is true for any independent, identically distributed random variables with a finite mean and variance. The central limit theorem is essential because it allows us to make inferences about a population based on a sample.\nThe basic equation for the central limit theorem is:\n\\(z = (x̄ - μ) / (σ / √n)\\)\nwhere \\(z\\) is the standard normal random variable, \\(x̄\\) is the sample mean, μ is the population mean, σ is the population standard deviation, and n is the sample size.\nA simulation plot of the central limit theorem can demonstrate how the sample mean distribution approaches a normal distribution as the sample size increases. The plot can show different sample sizes on the x-axis and the sample mean distribution on the y-axis. The simulation can be run using a non-normal population distribution, such as the uniform or exponential distribution, and show that as the sample size increases, the distribution of the sample means becomes increasingly normal. The plot can also illustrate how the standard deviation of the sample means decreases as the sample size increases, indicating more precise estimates of the population mean. This visualization can help to reinforce the concept of the central limit theorem and its practical implications in statistical analysis.\n\n\nknitr::include_graphics(\"CentralTheorm_ND_sim.png\")\n\n\n\nWhy do we need central theorm?\nIntuitively, we can think of the distribution of sample means as a way of characterizing the variability that we would expect to see in the sample means if we were to take many different samples from the same population. If we were to take multiple samples of the same size from a population and calculate the sample mean for each sample, we would end up with a range of different values. The distribution of these sample means would give us a sense of how much variability we can expect in the sample mean, which in turn provides information about the uncertainty in our estimate of the population mean.\nThe distribution of sample means is important because it allows us to make statements about the population mean with a certain degree of confidence. By calculating the standard error of the mean and constructing confidence intervals around the sample mean, we can estimate the range of values that the population mean is likely to fall within with a certain level of confidence. This is a powerful tool in statistical inference and is used in a wide range of fields, from medicine to finance to social sciences.\n\n\n\n",
      "last_modified": "2023-05-09T15:38:54+02:00"
    },
    {
      "path": "Stat_basics_lm.html",
      "title": "Basic concepts in Statistics",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHello and welcome to my blog post about linear models in statistics!\nLinear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables. They are widely used in fields such as economics, engineering, and social sciences to make predictions and understand the impact of one variable on another. In this post, we’ll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.\nBefore we dive into the Shiny app, let’s first define what a linear model is. A linear model is a mathematical equation that represents a linear relationship between two or more variables. The simplest form of a linear model is a straight line equation of the form:\n\\(y_{i} = \\beta_{0} + \\beta_{1} \\times X_{1}\\)\nWhere Y is the dependent variable, X is the independent variable, a is the intercept, and b is the slope of the line. The slope represents the change in Y for every unit increase in X, while the intercept is the value of Y when X is zero.\nLinear models can be extended to include more than one independent variable, and the equation becomes:\n\\(y{i} = a + \\beta_{1} \\times X_{1} + \\beta_{2} \\times X_{2} + … + \\beta_{n} \\times X_{n}\\)\nWhere Y is the dependent variable, X1, X2, …, Xn are the independent variables, a is the intercept, and b1, b2, …, bn are the slopes of the respective variables.\nNow let’s move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:\nChoose the variables from the dropdown menu.\nObserve how the line changes to fit the data points better.\nObserve how the changes in:\nIntercept\nSlope(s)\np-values\nR-squared value as you adjust the model.\nThe app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.\nThe data Im using here is from a built-in dataset in R called “mtcar”.\n\n\nlibrary(knitr)\nkable(head(mtcars))\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\nAfter looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model – the line with the lowest RSS)\n\n\n\n\nsomehting\n\n\n\n\nIn conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don’t hesitate to leave any comments or questions below!\n\n\n\n",
      "last_modified": "2023-05-09T12:45:43+02:00"
    },
    {
      "path": "Teaching.html",
      "author": [],
      "contents": "\ntitle: “Untitled”\ndescription: |\nA new article created using the Distill format.\nauthor:\n- name: Nora Jones\nurl: https://example.com/norajones\naffiliation: Spacely Sprockets\naffiliation_url: https://example.com/spacelysprokets\ndate: “2023-05-09”\noutput:\noutput: distill::distill_article\n\n\n\n",
      "last_modified": "2023-05-09T12:45:43+02:00"
    }
  ],
  "collections": []
}
