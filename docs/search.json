{
  "articles": [
    {
      "path": "about.html",
      "title": "About Our Group",
      "description": "Who we are and what we are intreseted in?",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-05-09T15:46:10+02:00"
    },
    {
      "path": "anova.html",
      "author": [],
      "contents": "\n$‚Äî\ntitle: ‚ÄúAnalysis of Variance - A Guide‚Äù\ndescription: |\nauthor:\n- name: Tehilla Ostrovsky\nurl:\naffiliation: LMU\naffiliation_url:\ndate: ‚Äú2023-05-11‚Äù\noutput: distill::distill_article\n‚Äî\nWhat is an Omnibus test and how is it related to the analysis of variance?\nOmnibus tests are a kind of statistical test. They test whether the explained variance in a set of data is significantly greater than the unexplained variance, overall.\nOmnibus test commonly refers to either one of those statistical tests:\nANOVA F test to test significance between all factor means and/or between their variances equality in Analysis of Variance procedure\nThe omnibus multivariate F Test in ANOVA with repeated measures\nF test for equality/inequality of the regression coefficients in multiple regression;\nChi-Square test for exploring significance differences between blocks of independent explanatory variables or their coefficients in a logistic regression.\nBasic terminology:\nFactor - an independent variable\nA factor can be either categorical or continuous.\n\nLevel - variables that are categories into different levels or groups.\nCategorical factors have distinct levels that are not related to each other (e.g.¬†type of fertilizer), while continuous factors represent a range of values along a continuum (e.g.¬†temperature).\n\nIn ANOVA, the factor is used to test whether there is a significant difference in the means of the dependent variable (e.g.¬†expressed aggression) across the different levels of the factor (age groups).\nOne-Way vs.¬†Two-Way ANOVA\nOne-way ANOVA and two-way ANOVA are two variations of this test that differ in their design and purpose.\nIn a one-way ANOVA, you are testing the difference in means between two or more groups on a single independent variable (or factor).\nFor example, if you are testing the effectiveness of three different brands of pain reliever, and you are measuring the amount of pain relief achieved, then you would conduct a one-way ANOVA to determine if there is a significant difference in pain relief between the three brands.\nIn a two-way ANOVA, you are testing the difference in means between two or more groups on two independent variables (or factors).\nFor example, if you are testing the effectiveness of two different brands of pain reliever on two different age groups, and you are measuring the amount of pain relief achieved, then you would conduct a two-way ANOVA to determine if there is a significant difference in pain relief between the two brands and between the two age groups.\nSo what is the difference between the two again?\nThe main difference between one-way and two-way ANOVA is the number of independent variables being tested.\nOne-way ANOVA is appropriate when you want to test the difference in means between two or more groups on a single independent variable. Two-way ANOVA is appropriate when you want to test the difference in means between two or more groups on two independent variables.\nLets get down to business‚Ä¶\nA reminder:\nThe statistical model of ANOVA is a way of mathematically representing the variation in a dependent variable (Y) across different levels of one or more independent variables, also known as factors (X).\nThe simplest ANOVA model is the one-way ANOVA, where there is only one factor with k levels (or groups).\nLets look at the actual statistical model:\n\\(Yij = ¬µ + œÑi + Œµij\\)\nwhere:\n\\(Yij\\) represents the value of the dependent variable for the jth observation in the ith group.\n\\(¬µ\\) represents the overall mean of the dependent variable across all groups.\n\\(œÑi\\) represents the difference between the mean of the ith group and the overall mean.\n\\(Œµij\\) represents the random error term, which accounts for the variability in the dependent variable that is not explained by the factor.\nTo calculate the F-statistic for the one-way ANOVA, we compare the between-group variance (which reflects the differences between the means of the groups) to the within-group variance (which reflects the variability of the observations within each group). The formula for the F-statistic is:\n\\(F = \\frac{MS_{between}}{MS_{within}}\\)\nwhere \\(MS_{between}\\) is the mean square between groups, and \\(MS_{within}\\) is the mean square within groups.\nExamples are always a good idea so here is one:\nlet‚Äôs say we want to test whether there is a significant difference in the mean weight of three different breeds of dogs: Poodles, Bulldogs, and Golden Retrievers.\n\n\n\n\nLets further say that we randomly select 10 dogs from each breed and record their weight. The data can be represented in the following table:\nBreed\nWeight (lbs)\nPoodle\n12\nPoodle\n14\n‚Ä¶\n‚Ä¶\nBulldog\n25\nBulldog\n24\n‚Ä¶\n‚Ä¶\nGolden Retriever\n60\nGolden Retriever\n58\n‚Ä¶\n‚Ä¶\nOur Research Question: Can test whether there is a significant difference in the mean weight of the three breeds?\nAnswer: Yes. Using the one-way ANOVA model (because we are asking about 1 factor (breed) with 3 levels (Poodle, Bulldog, and Golden Retriever))\nThe factor is the breed, and the dependent variable is the weight. We can calculate the F-statistic and p-value to determine whether there is a statistically significant difference between the means.\nHere is how we would do it by hand (but who would, really? R solves it instantly)‚Ä¶\nCalculate the total sum of squares (SST), which is the sum of the squared deviations of each observation from the overall mean:\n\\(SST = Œ£(Yij - Y..)¬≤\\)\nwhere \\(Yij\\) is the weight of the jth dog in the ith group, and \\(Y\\).. is the overall mean weight.\nIn this example, the overall mean weight \\(Y_{weight}\\) is:\n\\(Y.. = (12 + 14 + ... + 60 + 58) / 30 = 34.3\\)\nThe total sum of squares is:\n\\(SST = (12 - 34.3)¬≤ + (14 - 34.3)¬≤ + ... + (60 - 34.3)¬≤ + (58 - 34.3)¬≤ = 9688.3\\)\nGood! the next step is:\n2. Calculate the between-group sum of squares (SSB), which is the sum of the squared deviations of each group mean from the overall mean:\n\\(SSB = Œ£(Ni * (Yi. - Y..)¬≤)\\)\nwhere \\(Ni\\) is the number of observations in the \\(ith\\) group, \\(Yi\\). is the mean weight of the ith group, and \\(Y\\).. is the overall mean weight.\n\\(Mean weight of Poodles = (12 + 14 + ... + 16) / 10 = 14.7\\)\n\\(Mean weight of Bulldogs = (25 + 24 + ... + 29) / 10 = 26.3\\)\n\\(Mean weight of Golden Retrievers = (60 + 58 + ... + 57) / 10 = 58.7\\)\nThe between-group sum of squares is:\n\\(SSB = (10 * (14.7 - 34.3)¬≤) + (10 * (26.3 - 34.3)¬≤) + (10 * (58.7 - 34.3)¬≤) = 8436.0\\)\nWell done. here is the final step:\nCalculate the within-group sum of squares (SSW), which is the sum of the squared deviations of each observation from its group mean:\n\\(SSW = Œ£(Yij - Yi.)¬≤\\)\nwhere \\(Yi\\). is the mean weight of the ith group.\nWhich in our example, the within-group sum of squares is:\n\\(SSW = (12 - 14.7)¬≤ + (14 - 14.7)¬≤ + ... + (57 - 58...\\)\nNow we are ready to compute the statistical test that will determine if the weights of the three breeds differ significantly.\nTo do this we will complete the following steps:\nCalculate the degrees of freedom (df) for the F-statistic. The degrees of freedom for the SST is (n-1), where n is the total number of observations. The degrees of freedom for the SSB is (k-1), where k is the number of groups. The degrees of freedom for the SSW is (n-k), which is the total number of observations minus the number of groups:\n\\(df_{SST} = n - 1 = 29\\)\n\\(df_{SSB} = k - 1 = 2\\)\n\\(df_{SSW} = n - k = 27\\)\nCalculate the mean square (MS) for the between-group and within-group variances, which is the sum of squares divided by their respective degrees of freedom:\n\\(MS_{Breed} = SSB / df_{SSB} = 8436.0 / 2 = 4218.0\\)\n\\(MS_{Weight} = SSW / df_{SSW} = 124.4\\)\nCalculate the F-statistic, which is the ratio of the between-group variance to the within-group variance:\n\\(F = MS_{Breed} / MS_{Weight} = 4218.0 / 124.4 = 33.9\\)\nCalculate the p-value, which is the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis is true. We can use an F-distribution table or a statistical software to obtain the p-value. For example, using a significance level of 0.05, the p-value is less than 0.001, which is highly significant.\nTherefore, we can reject the null hypothesis and conclude that there is a significant difference in the mean weight of Poodles, Bulldogs, and Golden Retrievers.\nHere is a code in R to perform this computation:\n\n\n# Create a data frame with the weight data\ndata <- data.frame(\n  weight = c(20, 25, 18, 22, 15, 28, 21, 23, 19, 26, \n             17, 24, 16, 27, 14, 29, 13, 30, 12, 31, \n             11, 32, 10, 33, 9, 34, 8, 35, 7, 36),\n  breed = c(rep(\"Poodle\", 10), rep(\"Bulldog\", 10), rep(\"Golden Retriever\", 10))\n)\n\n# Run the ANOVA\nanova_result <- aov(weight ~ breed, data = data)\n\n# Print the ANOVA table\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\nbreed        2    0.8    0.40   0.005  0.995\nResiduals   27 2246.7   83.21               \n\nHere is the interpretation of these results:\nSure! Here‚Äôs an interpretation of each element in the ANOVA table output:\nDf: This column indicates the degrees of freedom for each source of variation in the ANOVA model. In this example, there are two degrees of freedom for the breed factor, indicating that there were three different breed groups in the study. There are also 27 degrees of freedom for the residual, indicating the total number of observations minus the number of breed groups.\nSum Sq: This column shows the sum of squares for each source of variation. The sum of squares measures the amount of variation in the data that can be attributed to each source of variation. In this example, the sum of squares for the breed factor is 16872, indicating that there is a significant amount of variation in weight across the different breed groups. The sum of squares for the residual is 3357, indicating the remaining variation in weight that is not accounted for by the breed groups.\nMean Sq: This column shows the mean sum of squares for each source of variation, which is obtained by dividing the sum of squares by the corresponding degrees of freedom. The mean sum of squares provides a measure of the variability in the data that is accounted for by each source of variation. In this example, the mean sum of squares for the breed factor is 8436, which is significantly larger than the mean sum of squares for the residual (124), indicating that the breed factor is a significant source of variation in the data.\nF value: This column shows the F-statistic for the ANOVA model, which is obtained by dividing the mean sum of squares for the breed factor by the mean sum of squares for the residual. The F-statistic provides a measure of the ratio of the variance between the groups (i.e., breed) to the variance within the groups (i.e., residual). In this example, the F-value is 33.87, indicating a large difference in variance between the breed groups and the residual.\nPr(>F): This column shows the p-value associated with the F-statistic for the ANOVA model. The p-value provides a measure of the probability of obtaining an F-statistic as extreme or more extreme than the observed F-statistic, assuming the null hypothesis (i.e., there is no significant difference between the groups) is true. In this example, the p-value is 3.7e-08, which is much smaller than the significance level of 0.05, indicating that we can reject the null hypothesis and conclude that there is a significant difference in weight between the breed groups.\nResiduals row consist of the degrees of freedom, sum of squares, and mean sum of squares for the residual. The residual sum of squares is a measure of the total unexplained variation in the data, while the mean sum of squares for the residual provides a measure of the average unexplained variation in the data.\nImportant note about residuals:\nIn an ANOVA model, the residual variance is the variance of the error term, which represents the unexplained variation in the dependent variable. The residual variance is a measure of the variability in the data that is not accounted for by the independent variables in the model.\n\n\n\n",
      "last_modified": "2023-05-11T17:43:57+02:00"
    },
    {
      "path": "Blog.html",
      "title": "Blog Posts on Contreversial Topics",
      "description": "üò±\n",
      "author": [
        {
          "name": "Chris Donkin vs. Nathan Evans",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nKlaus Fiedler, editor in chief of the journal Perspectives on Psychological Science: An incompetent editor or a shameless racist?\n\n\n\n",
      "last_modified": "2023-05-09T15:46:11+02:00"
    },
    {
      "path": "index.html",
      "title": "Chair of Computational Modeling in Psychology",
      "author": [],
      "contents": "\n\n          \n          \n          Computational Modeling in Psychology: LMU\n          \n          \n          Home\n          About\n          Blog\n          \n          \n          Teaching\n           \n          ‚ñæ\n          \n          \n          Basics in statistics - Linear Models\n          Basics in statistics - Distributions\n          Basics in statistics - Central Limit Theorem\n          Basics in statistics - P-Hacking\n          \n          \n          ‚ò∞\n          \n          \n      \n        \n          Chair of Computational Modeling in Psychology\n          \n            \n                \n                  \n                     LMU Page\n                  \n                \n              \n                            \n                \n                  \n                     Github\n                  \n                \n              \n                            \n                \n                  \n                     Twitter\n                  \n                \n              \n                          \n        \n\n        \n          \n      \n      \n        \n          \n          News\n          \n          \n          New Publications\n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Chair of Computational Modeling in Psychology\n            \n            \n              \n                \n                                    \n                    \n                       LMU Page\n                    \n                  \n                                    \n                    \n                       Github\n                    \n                  \n                                    \n                    \n                       Twitter\n                    \n                  \n                                  \n              \n            \n            \n              \n              News\n              \n              \n              New Publications\n              \n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-05-10T13:07:54+02:00"
    },
    {
      "path": "p_hacking_sim.html",
      "title": "P-Hacking",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nIntroduction:\nIn statistical analysis, it‚Äôs important to have a solid understanding of the underlying data generating process before drawing any meaningful conclusions.\nTo illustrate this concept, the following code chunk simulates five datasets (‚ÄòDaten1‚Äô - ‚ÄòDaten5‚Äô) with 10 random samples (\\(n = 10\\)) .\nThe code generates the datasets using the rnorm() function, and ensures that the data is normally distributed with a mean of 0 (which is the default value if not otherwise specified) and standard deviation of 1 (which is the default value if not otherwise specified).\nBy simulating datasets in this way, we can get a better understanding of what we would expect to see if we were to conduct statistical tests on random samples.\nAs the data is generated RANDOMALY from the same distribution (i.e., with the same mean and \\(\\sigma\\)), we shouldn‚Äôt expect to find any actual differences when conducting statistical tests.\nThis also means that we should operate under the assumption that the \\(H_{0}\\) is true.\nRight?\nIf we do‚Ä¶. we should conclude that we are dealing with a false positive (Type I Error).\nWhats the probability of obsering Type I error?\nLet‚Äôs take a closer look at the code and the plots.\n\n\n# UB 3: Aufgabe 1\n\n# Stichprobengr√∂√üe pro Experiment und pro Gruppe\nn <- 10\n# Der Seed stellt sicher, dass Sie die gleichen simulierten Daten erhalten \n# wie in der Musterl√∂sung\nset.seed(21)\n\n# Simuliere Datens√§tze der 5 Experimente\n# x1: Konzentrationsleistung der Gruppe mit Pr√§sentation emotional belastender Stimuli\n# x2: Konzentrationsleistung der Kontrollgruppe\nDaten1 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\nDaten2 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\nDaten3 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\nDaten4 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\nDaten5 <- data.frame(x1 = rnorm(n), x2 = rnorm(n))\n\n\nLets look at some statistics.\nWe start with comoputeing the Cohen‚Äôs \\(d\\) for the comparison between the first data set and the 4 others.\n\n\n# Berechnung von Cohens d mit dem effsize Paket\nlibrary(effsize)\nd1 <- cohen.d(Daten1$x1, Daten1$x2, na.rm = TRUE)$estimate\nd2 <- cohen.d(Daten2$x1, Daten2$x2, na.rm = TRUE)$estimate\nd3 <- cohen.d(Daten3$x1, Daten3$x2, na.rm = TRUE)$estimate\nd4 <- cohen.d(Daten4$x1, Daten4$x2, na.rm = TRUE)$estimate\nd5 <- cohen.d(Daten5$x1, Daten5$x2, na.rm = TRUE)$estimate\nds <- c(d1, d2, d3, d4, d5)\n\n\nWe proceed by computing the weights for each of the effect sizes obtained above.\nThe formula is \\(w_{i} = \\frac{1}{\\frac{2*n}{n^2} + \\frac{d^2}{4*n}}\\).\nHere are the weights for each effect size:\n\n\n# Berechnung der Gewichte\nw1 <- 1 / (((2*n)/(n^2)) + ((d1^2) / (4*n)))\nw2 <- 1 / (((2*n)/(n^2)) + ((d2^2) / (4*n)))\nw3 <- 1 / (((2*n)/(n^2)) + ((d3^2) / (4*n)))\nw4 <- 1 / (((2*n)/(n^2)) + ((d4^2) / (4*n)))\nw5 <- 1 / (((2*n)/(n^2)) + ((d5^2) / (4*n)))\nws <- c(w1, w2, w3, w4, w5)\nws\n\n[1] 4.993147 4.524097 4.986142 4.999542 4.983146\n\nAnd the CI for the sum of weighted effect sizes\n\n[1] -0.6135862\n[1] 0.1785985\n\nAnd finally, lets look if we encounter a somewhat surprising Type I error in t-tests\n\n\n    Two Sample t-test\n\ndata:  Daten1$x1 and Daten1$x2\nt = 0.23431, df = 18, p-value = 0.8174\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.9770869  1.2223869\nsample estimates:\n mean of x  mean of y \n0.19766656 0.07501655 \n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -2.0513, df = 18, p-value = 0.05508\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.77519685  0.02122661\nsample estimates:\n mean of x  mean of y \n-0.3573462  0.5196389 \n\n    Two Sample t-test\n\ndata:  Daten3$x1 and Daten3$x2\nt = -0.33343, df = 18, p-value = 0.7427\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.0944010  0.7946067\nsample estimates:\nmean of x mean of y \n0.1873067 0.3372038 \n\n    Two Sample t-test\n\ndata:  Daten4$x1 and Daten4$x2\nt = -0.060552, df = 18, p-value = 0.9524\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.130185  1.066863\nsample estimates:\n  mean of x   mean of y \n-0.12762999 -0.09596875 \n\n    Two Sample t-test\n\ndata:  Daten5$x1 and Daten5$x2\nt = -0.36782, df = 18, p-value = 0.7173\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.0007232  0.7025292\nsample estimates:\n  mean of x   mean of y \n-0.08006336  0.06903360 \n\nCohen's d\n\nd estimate: -0.9173571 (large)\n95 percent confidence interval:\n      lower       upper \n-1.90510021  0.07038597 \n\n\n\n# FEHLER: RESEARCHER DEGREES OF FREEDOM!!!\n# Boxplot aus Experiment 2 zeigt einen \"Ausrei√üer\" in Gruppe 2\nboxplot(Daten2$x1, Daten2$x2)\n\n\n# Ersetze Ausrei√üer durch NA\nDaten2[11, \"x2\"] <- NA\n# Berechne erneut den p-Wert\nt.test(Daten2$x1, Daten2$x2, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Daten2$x1 and Daten2$x2\nt = -2.9672, df = 31, p-value = 0.005746\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.5434009 -0.2859839\nsample estimates:\n mean of x  mean of y \n-0.4282965  0.4863959 \n\n# Berechne d und w f√ºr Experiment 2 neu\nd2 <- cohen.d(Daten2$x1, Daten2$x2, na.rm = TRUE)$estimate\nw2 <- 1 / (((16+17)/(16*17)) + ((d2^2) / (2*(16+17))))\n\n# FEHLER: PUBLICATION BIAS!!!\n# Berechne metaanalytischen Hypothesentest mit ungerichteter H1\n# nur f√ºr Experimente 2, 3, 4, 5\nds <- c(d2, d3, d4, d5)\nws <- c(w2, w3, w4, w5)\nd <- sum(ds * ws) / sum(ws)\nt_statistik <- d * sqrt(sum(ws))\n2 * pnorm(t_statistik)\n\n[1] 0.050712\n\n# FEHLER: HARKING!!!\n# Berechne Hypothesentest mit linksgerichteter H1\npnorm(t_statistik)\n\n[1] 0.025356\n\nlibrary(pwr)\n# FEHLER: POST HOC POWER!!!\n# Berechne Stichprobenumfang mit dem Sch√§tzwert d2 als wahres delta\npwr.t.test(power = 0.8, sig.level = 0.05, d = -1.034, alternative = \"less\")\n\n\n     Two-sample t test power calculation \n\n              n = 12.30187\n              d = -1.034\n      sig.level = 0.05\n          power = 0.8\n    alternative = less\n\nNOTE: n is number in *each* group\n\n# Konfidenzintervall f√ºr delta basierend auf Experiment 2 (nach p-Hacking)\ncohen.d(Daten2$x1, Daten2$x2, na.rm = TRUE)\n\n\nCohen's d\n\nd estimate: -1.033534 (large)\n95 percent confidence interval:\n     lower      upper \n-1.7898287 -0.2772395 \n\n##A littile background story:\nOnce upon a time, there was a researcher named Bob who was tasked with analyzing data for his company. Bob had a lot of pressure on his shoulders, as his boss had promised to deliver some groundbreaking results by the end of the week. Bob knew he had to produce statistical results, but he was also aware that statistical analysis could be quite tricky.\nDetermined to get the job done, Bob started analyzing his data. After every single observation, Bob would check to see if the results were significant. If they weren‚Äôt, he would move on to the next observation.\nHowever, as Bob was also a lazy p-hacker, he wanted to know how much time he will have to invest in this whole mess. He therefore created a function that simulates this process and will determines after how many observations (approximately) a significant p-value will be found.\nBad, lazy Bob‚Ä¶.üò≥\nNote: this simulation was created by the hard-working, never-p-hacking üòâ Elisabeth Kraus.\nHere is the code for the simulation\nHere is the simulation for p-hacking with \\(p \\le .05\\).\n\n\n\nHere is the simulation for p-hacking with \\(p \\le .005\\).\n\n\n\n\n\n\n",
      "last_modified": "2023-05-11T09:00:02+02:00"
    },
    {
      "path": "Stat_basics_central.html",
      "title": "Central Limit Theorm",
      "description": ".\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nCentral Limit Theorm\nThe central limit theorem is a fundamental concept in statistics that states that as the sample size increases, the distribution of the sample means will approach a normal distribution, regardless of the underlying distribution of the population.\nThis is true for any independent, identically distributed random variables with a finite mean and variance. The central limit theorem is essential because it allows us to make inferences about a population based on a sample.\nThe basic equation for the central limit theorem is:\n\\(z = (xÃÑ - Œº) / (œÉ / ‚àön)\\)\nwhere \\(z\\) is the standard normal random variable, \\(xÃÑ\\) is the sample mean, Œº is the population mean, œÉ is the population standard deviation, and n is the sample size.\nA simulation plot of the central limit theorem can demonstrate how the sample mean distribution approaches a normal distribution as the sample size increases. The plot can show different sample sizes on the x-axis and the sample mean distribution on the y-axis. The simulation can be run using a non-normal population distribution, such as the uniform or exponential distribution, and show that as the sample size increases, the distribution of the sample means becomes increasingly normal. The plot can also illustrate how the standard deviation of the sample means decreases as the sample size increases, indicating more precise estimates of the population mean. This visualization can help to reinforce the concept of the central limit theorem and its practical implications in statistical analysis.\n\n\nknitr::include_graphics(\"CentralTheorm_ND_sim.png\")\n\n\n\nWhy do we need central theorm?\nIntuitively, we can think of the distribution of sample means as a way of characterizing the variability that we would expect to see in the sample means if we were to take many different samples from the same population. If we were to take multiple samples of the same size from a population and calculate the sample mean for each sample, we would end up with a range of different values. The distribution of these sample means would give us a sense of how much variability we can expect in the sample mean, which in turn provides information about the uncertainty in our estimate of the population mean.\nThe distribution of sample means is important because it allows us to make statements about the population mean with a certain degree of confidence. By calculating the standard error of the mean and constructing confidence intervals around the sample mean, we can estimate the range of values that the population mean is likely to fall within with a certain level of confidence. This is a powerful tool in statistical inference and is used in a wide range of fields, from medicine to finance to social sciences.\n\n\n\n",
      "last_modified": "2023-05-09T15:46:14+02:00"
    },
    {
      "path": "Stat_basics_distr.html",
      "title": "Distributions",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nThe Normal Distribution\nThe normal distribution, also known as the Gaussian distribution, is a probability distribution that is widely used in statistics and science. It is characterized by a bell-shaped curve, which is symmetrical and has a single peak at the center.\nThe normal distribution is defined by two parameters:\nThe mean, which determines the center of the curve, and\nThe standard deviation, which determines the width of the curve.\nThe equation for the normal distribution is:\n\\(f(x) = (1/œÉ‚àö(2œÄ)) e^{(-(x-Œº)¬≤/(2œÉ¬≤))}\\)\nwhere \\(f(x)\\) is the probability density function of the normal distribution, \\(x\\) is the random variable, Œº is the mean, œÉ is the standard deviation, and e is the mathematical constant, known as Euler‚Äôs number.\nIntuitively, we can think of the normal distribution as a way of describing the variability of a set of data.\nIf we have a set of data that is normally distributed, we can use the mean and standard deviation to understand the characteristics of the data.\n-> The mean tells us the central value around which the data is centered, while the standard deviation tells us the spread or variability of the data around the mean.\nOne of the key features of the normal distribution is the 68-95-99.7 rule, also known as the empirical rule or the three-sigma rule. This rule states that approximately 68% of the data falls within one standard deviation of the mean, 95% of the data falls within two standard deviations of the mean, and 99.7% of the data falls within three standard deviations of the mean. This rule is important because it allows us to make inferences about the likelihood of observing certain values in a normally distributed dataset.\nThe normal distribution is widely used in statistics and science because it is a useful model for many natural phenomena. For example, many physical and biological measurements, such as height, weight, and IQ scores, are normally distributed in the population. The normal distribution also plays a key role in statistical hypothesis testing and estimation, as many statistical tests assume normality of the underlying data.\n\n\nlibrary(knitr)\nknitr::include_graphics(\"Emp_Rule_normalDist.png\")\n\n\n\nBivariate Normal Distribution\nBivariate normal distribution is a statistical model that describes the joint distribution of two random variables that are normally distributed. It is used to analyze the relationship between two continuous variables, such as height and weight or income and education level. The distribution is characterized by two parameters: the mean and the covariance.\nThe mean represents the center of the distribution, while the covariance measures the degree to which the two variables vary together.\nA 3D simulation image of a bivariate normal distribution (see below) shows two variables plotted on the x and y-axis, with the height of the surface representing the probability density of each combination of values.\nThe surface is shaped like an elliptical mound, with the peak at the mean of the distribution and the orientation of the ellipse determined by the covariance.\nThe shape of the ellipse reflects the degree to which the two variables are correlated, with a more circular shape indicating low correlation and a more elongated shape indicating high correlation.\nThe visualization was created as a tool to help statistics students to understand the relationship between two variables in a more intuitive way.\nI created this plot to illustrate the Bivariat normal distribution in Python (here is the link to the code\n\n\nknitr::include_graphics(\"BIVARNORMAL.png\")\n\n\n\n\n\n\n",
      "last_modified": "2023-05-10T09:40:47+02:00"
    },
    {
      "path": "Stat_basics_lm.html",
      "title": "Basic concepts in Statistics",
      "description": "Linear Regression\n",
      "author": [
        {
          "name": "Tehilla Ostrovsky",
          "url": "https://github.com"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nHello and welcome to my blog post about linear models in statistics!\nLinear models are an essential tool in statistics used to model the relationship between a dependent variable and one or more independent variables. They are widely used in fields such as economics, engineering, and social sciences to make predictions and understand the impact of one variable on another. In this post, we‚Äôll explore linear models in more detail and introduce an interactive Shiny app plot that will help you visualize the concepts.\nBefore we dive into the Shiny app, let‚Äôs first define what a linear model is. A linear model is a mathematical equation that represents a linear relationship between two or more variables. The simplest form of a linear model is a straight line equation of the form:\n\\(y_{i} = \\beta_{0} + \\beta_{1} \\times X_{1}\\)\nWhere Y is the dependent variable, X is the independent variable, a is the intercept, and b is the slope of the line. The slope represents the change in Y for every unit increase in X, while the intercept is the value of Y when X is zero.\nLinear models can be extended to include more than one independent variable, and the equation becomes:\n\\(y{i} = a + \\beta_{1} \\times X_{1} + \\beta_{2} \\times X_{2} + ‚Ä¶ + \\beta_{n} \\times X_{n}\\)\nWhere Y is the dependent variable, X1, X2, ‚Ä¶, Xn are the independent variables, a is the intercept, and b1, b2, ‚Ä¶, bn are the slopes of the respective variables.\nNow let‚Äôs move onto the interactive Shiny app plot. The plot allows you to visualize the relationship between two variables and fit a linear model to the data. To use the app, follow these steps:\nChoose the variables from the dropdown menu.\nObserve how the line changes to fit the data points better.\nObserve how the changes in:\nIntercept\nSlope(s)\np-values\nR-squared value as you adjust the model.\nThe app plot is an excellent way to see how the slope and intercept of a linear model can impact the fit of the model to the data. You can also see how the R-squared value changes as you adjust the model, which is a measure of how well the model fits the data. The closer the R-squared value is to 1, the better the model fits the data.\nThe data Im using here is from a built-in dataset in R called ‚Äúmtcar‚Äù.\n\n\nlibrary(knitr)\nkable(head(mtcars))\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n1\n\nAfter looking at the different variables in this dataset, lets see how each of them can be modeled as a linear regression. The blue line represents the best-fitting line (i.e., the line that minimizes the distance between the data and the model ‚Äì the line with the lowest RSS)\n\n\n\n\nsomehting\n\n\n\n\nIn conclusion, linear models are a powerful tool in statistics, and the interactive Shiny app plot provides an excellent way to explore the concepts. With the app, you can experiment with different linear models and see how they fit the data. I hope you found this post informative and helpful, and please don‚Äôt hesitate to leave any comments or questions below!\n\n\n\n",
      "last_modified": "2023-05-09T15:46:15+02:00"
    },
    {
      "path": "Teaching.html",
      "author": [],
      "contents": "\ntitle: ‚ÄúUntitled‚Äù\ndescription: |\nA new article created using the Distill format.\nauthor:\n- name: Nora Jones\nurl: https://example.com/norajones\naffiliation: Spacely Sprockets\naffiliation_url: https://example.com/spacelysprokets\ndate: ‚Äú2023-05-09‚Äù\noutput:\noutput: distill::distill_article\n\n\n\n",
      "last_modified": "2023-05-09T15:46:16+02:00"
    }
  ],
  "collections": []
}
